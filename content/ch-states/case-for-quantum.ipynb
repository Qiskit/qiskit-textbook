{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Case for Quantum Computers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [The Complexity of Adding](#adding)    \n",
    "2. [Big O Notation](#big-o)    \n",
    "3. [Complexity theory](#complexity)    \n",
    "4. [Beyond Digital Computation](#beyond)       \n",
    "5. [When to Use a Quantum Computer](#when)\n",
    "6. [References](#references) \n",
    "\n",
    "## 1. The Complexity of Adding <a id=\"adding\"></a>\n",
    "\n",
    "The case for quantum computers, simply put, is that they can solve certain problems that no other computational hardware ever could. To understand why this is, we need to be able to concretely discuss exactly how much computational effort is required to solve certain problems.\n",
    "\n",
    "To begin, we can revisit the algorithm considered in the first section: that for adding two numbers.\n",
    "\n",
    "```\n",
    "   9213\n",
    "+  1854\n",
    "=  ????\n",
    "```\n",
    "\n",
    "Adding two $n$ digit numbers can be done with a set of simple operations, each of which consists of just adding two single digit numbers. To analyze the complexity of the procedure we can think about how many of these basic additions are required, and how this number depends on $n$. We'll refer to this number as $c(n)$.\n",
    "\n",
    "In the easiest case, where don't need to carry a 1 at any point, only $n$ basic additions are required. In the worst case we will need to perform $n$ carry operations, each of which will require an extra basic addition. From these considerations, we can conclude that $n \\leq c(n) \\leq 2n$.\n",
    "\n",
    "## 2. Big O Notation <a id=\"big-o\"></a>\n",
    "\n",
    "We can summarize this result by saying that $c(n)$ grows linearly with $n$. More generally, we can that that a linear function with $n$ can be found which acts as an upper bound for $c(n)$ when $n$ is large. Since this is a long and wordy sentence, we won't actually want to say this very often. Instead, we can express it more compactly using 'big O notation'.\n",
    "\n",
    "<p>\n",
    " <details>\n",
    "  <summary>Definition: Big O notation</summary>\n",
    "\n",
    "    For some example functions $f(x)$ and $g(x)$ and parameter $x$, the statement $f(x) = O(g(x))$  means that there exists a finite numbers $M>0$ and $x_0$ such that\n",
    "    $$\n",
    "    g(x) \\leq M f(x) \\forall x>x_0.\n",
    "    $$ \n",
    " </details>\n",
    "</p>\n",
    "\n",
    "With this notation, the property described above is expressed simply as $c(n) = O(n)$. This captures the linear behaviour without needing to dwell on the specifics. So whether $c(n) = n$, or $c(n) = 2n$, or something in between, we can simply say that $c(n) = O(n)$.\n",
    "\n",
    "There is a hidden assumption in what we have considered so far. By talking about the number of digits, we have assumed the use of a specific number system. However, the number of digits will depend on which number system we are using, be it decimal, binary or something else. For example, the number of bits $n_2$ required to express a number is related to the number of decimal digits $n_{10}$ require to expressed the same number by\n",
    "\n",
    "$n_2 = \\left\\lceil \\frac{\\log 10}{ \\log 2} \\, n_{10} \\right\\rceil \\approx 3.3 \\, n_{10}.$\n",
    "\n",
    "Since this too is a linear relationship, it does not change how we express the complexity using big O notation. We can equally say that $c(n_2) = O(n_2)$, $c(n_{10}) = O(n_{10})$ or even $c(n_{10}) = O(n_{2})$. It is for this reason that we can often simply speak of the number of digits, $n$, without needing to specify what number system is used.\n",
    "\n",
    "\n",
    "## 3. Complexity theory <a id=\"complexity\"></a>\n",
    "\n",
    "Complexity theory is the study of the computational effort required to run any algorithm. By considering the best possible algorithm to solve a given problem, we can also study the computational effort inherent in solving this problem. For addition we already know the optimal algorithm, and so know that it is a problem with $O(n)$ complexity.\n",
    "\n",
    "Multiplication is not quite so simple. Algorithms you learned at school for multiplying two $n$ digit numbers will have required $O(n^2)$ basic operations, such as single digit additions and multiplications. Though algorithms with lower asymptotic complexity have been found, it is widely regarded as impossible to perform multiplication with $O(n)$ complexity.\n",
    "\n",
    "Even so, multiplication is far from being the most complex proble, to solve. An example of a problem with far greater complexity is factorization: taking an $n$ digit number and finding its prime factors. The best known algorithm in this case has a complexity that is worse than $O\\left(e^{n^{1/3}}\\right)$. The exponential here means that the complexity grows very quikly, and makes factorization a very hard problem to solve.\n",
    "\n",
    "To demonstrate this point using actual computation time, we can take a recent example$^{1}$. Consider the following 829 digit number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsa_250 = 2140324650240744961264423072839333563008614715144755017797754920881418023447140136643345519095804679610992851872470914587687396261921557363047454770520805119056493106687691590019759405693457452230589325976697471681738069364894699871578494975937497937"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you try using your computer to add or multiple numbers of this size, you'll find that it can solve such problems very quickly. If you multiply the number of processors your computer has with the number of seconds it takes to get the number of core-seconds, you are sure to find that very much less than 1 core-second is required.\n",
    "\n",
    "However, peforming factorization on this number required a supercomputer and around 2700 core-years! All to find that the number has the following two factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2140324650240744961264423072839333563008614715144755017797754920881418023447140136643345519095804679610992851872470914587687396261921557363047454770520805119056493106687691590019759405693457452230589325976697471681738069364894699871578494975937497937"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64135289477071580278790190170577389084825014742943447208116859632024532344630238623598752668347708737661925585694639798853367* 33372027594978156556226010605355114227940760344767554666784520987023841729210037080257448673296881877565718986258036932062711"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the factorization of much larger numbers we'll easily get to a point where a planet sized supercomputer would need to run for the age of the universe. Clearly any such problem is practically impossible, and simply beyond our means to solve.\n",
    "\n",
    "So far we have considered only mathematical operations on $n$ digit numbers, with complexity expressed as the number of simple single digit operations required. However, complexity theory can be used to analyze any computational method for any kind of problem, be it searching databases, rendering graphics, simulating dynamics or traversing a dungeon in *Legend of Zelda*. In each case we will be able to find a parameter or set of parameters that serve as our input size, and express the complexity in terms of this input size using big O notation. For searching a database of $N$ entries, for example, the complexity is $O(N)$.\n",
    "\n",
    "Formally, defining the compexity of an algorithm depends on the exact theoretical model for computation we are using. Each model has a set of basic operations, known as primitive operations, with which any algorithm can be expressed. For Boolean circuits, as we considered in the first section, the primitive operations are the logic gates. For Turing machines, a hypothetical form of computer proposed by Alan Turing, we imagine the device stepping through and manipulating information stored on a tape. The RAM model has a more complex set of primitive operations, and acts as an idealized form of the computers we use every day. All these are models of digital computation: based on discretized manipulations of discrete values. Different as they many seem from each other, it turns out that it is very easy for each of them to simulate the others. This means that the computational complexity typically doesn't depend much on which of these models is used. Rather than stating complexity specifically for the RAM model or Turing machines, we can therefore simply speak of the complexity for digital computers.\n",
    "\n",
    "## 4. Beyond Digital Computation <a id=\"beyond\"></a>\n",
    "\n",
    "Though digital computers are dominant now, they are not the only form of computation. Analog computers were also widely studied and used in the past. Unlike the discrete values of digital computers, these are based on precise manipulations of continuously varying paramters. It has sometimes been claimed that such devices could quickly solve problems that are intractable for digital computers. However, such claims have never yet been realized. A major stumbling block for analog computers is the inability to build devices with arbitrarily high precision. In digital computers, the discretization means that errors must be relatively large in order to be noticeable, and methods for detecting and correcting such errors can then be implemented. In analog computers, however, errors can be arbitrarily small and impossible to detect, but still their effects can build up to ruin a computation.\n",
    "\n",
    "If one were to propose an ideal model of computation, it might seek to combine the robustness of a digital computer with the subtle manipulations of an analog computer. To acheive this we can look to quantum mechanics. We have already seen that qubits are a system with discrete outputs `0` and `1`, and yet can exist in states that can only be described by continuous parameters. This is particular instance of the well-known notion of 'wave-particle' duality that is typical of quantum systems. They cannot be fully described as either discrete or continuous, but rather a combination of the two. As Einstein said$^{2}$\n",
    "\n",
    "> It seems as though we must use sometimes the one theory and sometimes the other, while at times we may use either. We are faced with a new kind of difficulty. We have two contradictory pictures of reality; separately neither of them fully explains the phenomena...but together they do.\n",
    "\n",
    "A quantum computer, whose primitive operations are gates applied to qubits, will therefore be neither analog nor digital, but something unique. In further chapters we will explore the consequences of this unique nature. We will see that quantum computers can solve problems with a radically different complexity to digital computers. And we will see that this can be done using techniques for quantum error correction, which can remove the effects of any imperfections.\n",
    "\n",
    "\n",
    "## 5. When to Use a Quantum Computer <a id=\"when\"></a>\n",
    "\n",
    "With qubits and quantum gates, we can design algorithms in ways that would be impossible for digital or analog computers. In this way, we have hope to find solutions to problems that are intractable for either of these forms of classical computational device.\n",
    "\n",
    "One way in which this can be done is when we have some function for which we want to determine a global property. For example, if we want to find the value of some parameter $x$ for which some function $f(x)$ is minimum, or the period of the function if $f(x)$ is periodic. An algorithm on a digital computer might use a process in which $f(x)$ is computed for a variety of different inputs, in order to get sufficient information about the global property. With a quantum computer, however, the fact that we can create superposition states means that the function can be applied to many possible inputs simultaneously. This does not mean that we can access all possible outputs, since measurement of such a state would simply give us one chosen at random. However, we can instead seek to induce a quantum interference effect, which will reveal the global property we require.\n",
    "\n",
    "This general description summarizes many of the quantum algorithms that have already been discovered. One prominent example is Grover's algorithm, which reduces the complexity of searching through $N$ items from $O(N)$ to $O(N^{1/2})$. This quadratic speedup could be useful in many applications with tasks that can be expressed as an unstructered search, such as optimization problems and machine learning.\n",
    "\n",
    "A more impressive speedup is obtained by Shor's algorithm, which analyses period functions at the heart of the factorization problem. This allows a quantum solution for factoring $n$ digit numbers with complexity $O(n^3)$. This is a superpolynomial speedup compared to the complexity for digital computers, which is worse than $O\\left(e^{n^{1/3}}\\right)$.\n",
    "\n",
    "Another route towards quantum algorithms is to use quantum computers to solve quantum problems. As we will see in the next chapter, expressing a quantum state requires an amount of information that scales exponentially with the number of qubits. Just writing down the state of $n$ qubits can therefore become an intractable task for digital computers as $n$ increased. However, for a quantum computer we just need $n$ qubits to do the same job. This natural capability to express and manipulate quantum states will allow us to study quantum systems of interest, such as molecules or interactions between fundamental particles. Quantum computers therefore promise to enhance our ability to discover new drugs and materials, and make scientific breakthroughs.\n",
    "\n",
    "This is just a taste of how quantum algorithms can peform computation in a unique way. More details on all these approaches can be found in later chapters. But first we need to look beyond the single qubit, and invest some time into understanding the full set of quantum gates that we will need. This will be the focus of the next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. References <a id='references'></a>\n",
    "\n",
    "1. https://lists.gforge.inria.fr/pipermail/cado-nfs-discuss/2020-February/001166.html\n",
    "2. Albert Einstein, Leopold Infeld (1938). The Evolution of Physics: The Growth of Ideas from Early Concepts to Relativity and Quanta. Cambridge University Press."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
