{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Case for Quantum Computers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [The Complexity of Adding](#adding)    \n",
    "2. [Big O Notation](#big-o)    \n",
    "3. [Complexity theory](#complexity)    \n",
    "4. [Beyond Digital Computation](#beyond)       \n",
    "5. [When to Use a Quantum Computer](#when)\n",
    "6. [References](#references) \n",
    "\n",
    "## 1. The Complexity of Adding <a id=\"adding\"></a>\n",
    "\n",
    "The case for quantum computers, simply put, is that they can solve certain problems that no other computational hardware ever could. To understand why this is, we need to be able to concretely discuss exactly how much computational effort is required to solve certain problems.\n",
    "\n",
    "To begin, we can revisit the algorithm considered in the first section: that for adding two numbers.\n",
    "\n",
    "```\n",
    "   9213\n",
    "+  1854\n",
    "=  ????\n",
    "```\n",
    "\n",
    "Adding two $n$ digit numbers can be done with a set of simple operations, each of which consists of just adding two single digit numbers. To analyze the complexity of the procedure we can think about how many of these basic additions are required, and how this number depends on $n$. We'll refer to this number as $c(n)$.\n",
    "\n",
    "In the easiest case, where don't need to carry a 1 at any point, only $n$ basic additions are required. In the worst case we will need to perform $n$ carry operations, each of which will require an extra basic addition. From these considerations, we can conclude that $n \\leq c(n) \\leq 2n$.\n",
    "\n",
    "## 2. Big O Notation <a id=\"big-o\"></a>\n",
    "\n",
    "We can summarize this result by saying that $c(n)$ grows linearly with $n$. More generally, we can that that a linear function with $n$ can be found which acts as an upper bound for $c(n)$ when $n$ is large. Since this is a long and wordy sentence, we won't actually want to say this very often. Instead, we can express it more compactly using 'big O notation'.\n",
    "\n",
    "<p>\n",
    " <details>\n",
    "  <summary>Definition: Big O notation</summary>\n",
    "\n",
    "    For some example functions $f(x)$ and $g(x)$ and parameter $x$, the statement $f(x) = O(g(x))$  means that there exists a finite numbers $M>0$ and $x_0$ such that\n",
    "    $$\n",
    "    g(x) \\leq M f(x) \\forall x>x_0.\n",
    "    $$ \n",
    " </details>\n",
    "</p>\n",
    "\n",
    "With this notation, the property described above is expressed simply as $c(n) = O(n)$. This captures the linear behaviour without needing to dwell on the specifics. So whether $c(n) = n$, or $c(n) = 2n$, or something in between, we can simply say that $c(n) = O(n)$.\n",
    "\n",
    "There is a hidden assumption in what we have considered so far. By talking about the number of digits, we have assumed the use of a specific number system. However, the number of digits will depend on which number system we are using, be it decimal, binary or something else. For example, the number of bits $n_2$ required to express a number is related to the number of decimal digits $n_{10}$ require to expressed the same number by\n",
    "\n",
    "$n_2 = \\left\\lceil \\frac{\\log 10}{ \\log 2} \\, n_{10} \\right\\rceil \\approx 3.3 \\, n_{10}.$\n",
    "\n",
    "Since this too is a linear relationship, it does not change how we express the complexity using big O notation. We can equally say that $c(n_2) = O(n_2)$, $c(n_{10}) = O(n_{10})$ or even $c(n_{10}) = O(n_{2})$. It is for this reason that we can often simply speak of the number of digits, $n$, without needing to specify what number system is used.\n",
    "\n",
    "\n",
    "## 3. Complexity theory <a id=\"complexity\"></a>\n",
    "\n",
    "Complexity theory is the study of the computational effort required to run any algorithm. By considering the best possible algorithm to solve a given problem, we can also study the computational effort inherent in solving this problem. For addition we already know the optimal algorithm, and so know that it is a problem with $O(n)$ complexity.\n",
    "\n",
    "Multiplication is not quite so simple. Algorithms you learned at school for multiplying two $n$ digit numbers will have required $O(n^2)$ basic operations, such as single digit additions and multiplications. Though algorithms with lower asymptotic complexity have been found, it is widely regardede as impossible to perform multiplication with $O(n)$ complexity.\n",
    "\n",
    "Even so, multiplication is far from being the most complex proble, to solve. An example of a problem with far greater complexity is factorization: taking an $n$ digit number and finding its prime factors. The best known algorithm in this case has a complexity that is worse than $O(e^{n^{1/3}})$. The exponential here means that the complexity grows very quikly, and makes factorization a very hard problem to solve.\n",
    "\n",
    "To demonstrate this point using actual computation time, we can take a recent example$^{1}$. Consider the following 829 digit number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsa_250 = 2140324650240744961264423072839333563008614715144755017797754920881418023447140136643345519095804679610992851872470914587687396261921557363047454770520805119056493106687691590019759405693457452230589325976697471681738069364894699871578494975937497937"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you try using your computer to add or multiple numbers of this size, you'll find that it can solve such problems very quickly. If you multiply the number of processors your computer has with the number of seconds it takes to get the number of core-seconds, you are sure to find that very much less than 1 core-second is required.\n",
    "\n",
    "However, peforming factorization on this number required a supercomputer and around 2700 core-years! All to find that the number has the following two factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2140324650240744961264423072839333563008614715144755017797754920881418023447140136643345519095804679610992851872470914587687396261921557363047454770520805119056493106687691590019759405693457452230589325976697471681738069364894699871578494975937497937"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64135289477071580278790190170577389084825014742943447208116859632024532344630238623598752668347708737661925585694639798853367* 33372027594978156556226010605355114227940760344767554666784520987023841729210037080257448673296881877565718986258036932062711"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the factorization of much larger numbers we'll easily get to a point where a planet sized supercomputer would need to run for the age of the universe. Clearly any such problem is practically impossible, and simply beyond our means to solve.\n",
    "\n",
    "So far we have considered only mathematical operations on $n$ digit numbers, with complexity expressed as the number of simple single digit operations required. However, complexity theory can be used to analyze any computational method for any kind of problem, be it searching databases, rendering graphics, simulating dynamics or traversing a dungeon in *Legend of Zelda*. In each case we will be able to find a parameter or set of parameters that serve as our input size, and express the complexity in terms of this input size using big O notation. For searching a database of $N$ entries, for example, the complexity is $O(N)$.\n",
    "\n",
    "Formally, defining the compexity of an algorithm depends on the exact theoretical model for computation we are using. Each model has a set of basic operations, known as primitive operations, with which any algorithm can be expressed. For Boolean circuits, as we considered in the first section, the primitive operations are the logic gates. For Turing machines, a hypothetical form of computer proposed by Alan Turing, we imagine the device stepping through and manipulating information stored on a tape. The RAM model has a more complex set of primitive operations, and acts as an idealized form of the computers we use every day. All these are models of digital computation: based on discretized manipulations of discrete values. Different as they many seem from each other, it turns out that it is very easy for each of them to simulate the others. This means that the computational complexity typically doesn't depend much on which of these models is used. Rather than stating complexity specifically for the RAM model or Turing machines, we can therefore simply speak of the complexity for digital computers.\n",
    "\n",
    "## 4. Beyond Digital Computation <a id=\"beyond\"></a>\n",
    "\n",
    "There are other forms of computation for which the complexity can be different. One example is that of analogue computers, which are based on arbitrarily precise manipulations of continuously varying paramters. In theory, such devices could easily solve some of the problems that are hard for digital computers. In practice, however, it is impossible to build such devices with arbitrarily high precision. In digital computers, the discretization means that errors must be relatively large in order to be noticeable, and methods for detecting and correcting such errors can then be implemented. In analog computers, however, errors can be arbitrarily small and impossible to detect, but nevertheless their effects can build up to ruin a computation.\n",
    "\n",
    "If one were to propose an ideal model of computation, it might seek to combine the robustness of a digital computer with the subtle manipulations of an analog computer. To acheive this we can look to quantum mechanics. We have already seen that qubits are a system with discrete outputs `0` and `1`, and yet can exist in states that can only be described by continuous parameters. This is particular instance of the well-known notion of 'wave-particle' duality that is typical of quantum systems. They cannot be fully described as either discrete or continuous, but rather a combination of the two. As Einstein said$^{2}$\n",
    "\n",
    "> It seems as though we must use sometimes the one theory and sometimes the other, while at times we may use either. We are faced with a new kind of difficulty. We have two contradictory pictures of reality; separately neither of them fully explains the phenomena...but together they do.\n",
    "\n",
    "\n",
    "A quantum computer, whose primitive operations are manipulations of qubits, will therefore be neither analog nor digital, but something unique. In further chapters we will explore the consequences of this unique nature. We will see that quantum computers can solve problems with a radically different complexity to digital computers. And we will see that this can be done using techniques for quantum error correction, which can remove the effects of any imperfections.\n",
    "\n",
    "\n",
    "## 5. When to Use a Quantum Computer <a id=\"when\"></a>\n",
    "\n",
    "For many kinds of computational problem, our familiar digital computers already offer the best that can be done. For some important cases, however, we know that quantum algorithms can allow otherwise intractable problems to be solved.\n",
    "\n",
    "One example is factorization. Rather than a complexity that is greater than $O(e^{n^{1/3}})$, Shor's algorithm offers a quantum solution with a complexity of $O(n^3)$.\n",
    "\n",
    "**continue**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. References <a id='references'></a>\n",
    "\n",
    "1. https://lists.gforge.inria.fr/pipermail/cado-nfs-discuss/2020-February/001166.html\n",
    "2. Albert Einstein, Leopold Infeld (1938). The Evolution of Physics: The Growth of Ideas from Early Concepts to Relativity and Quanta. Cambridge University Press."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
